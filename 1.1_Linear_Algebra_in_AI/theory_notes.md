# 📘 人工智能数学基础笔记  
## —— 标量、向量、矩阵与张量

---

## 1. 标量（Scalar）

标量是**只有大小、没有方向**的量。  
它是一个单独的数，可以是整数、实数或复数。  
常见例子：温度、时间、质量等。

在数学运算中，标量可以进行常规的四则运算：
- 加法 (+)
- 减法 (−)
- 乘法 (×)
- 除法 (÷)

---

## 2. 向量（Vector）

### 2.1 表现形式

向量是**既有大小又有方向**的量，可以形象化地表示为带箭头的线段。  
- 箭头方向 → 向量方向  
- 线段长度 → 向量大小  

在数学上通常表示为 **粗体字母** 或 **带箭头的字母**（如：**v** 或 $\vec{a}$）。  
在二维平面中可写作：  
$$
\vec{a} = (2, 3)
$$

---

### 2.2 向量的加减法

当两个向量进行加减时，必须是**同维向量**（即分量数量相同）。

设：
$$
\vec{a} = (a_1, a_2), \quad \vec{b} = (b_1, b_2)
$$

则：
$$
\vec{a} + \vec{b} = (a_1 + b_1, a_2 + b_2)
$$
$$
\vec{a} - \vec{b} = (a_1 - b_1, a_2 - b_2)
$$

示例：
$$
\vec{a} = (1, 2), \ \vec{b} = (3, 4)
$$  
则：
$$
\vec{a} + \vec{b} = (4, 6)
$$

---

### 2.3 向量的乘法

向量乘法分为两类：
1. 标量 × 向量（数乘）  
2. 向量 × 向量（内积 / 外积）

---

#### 2.3.1 向量的数乘

数乘会改变向量的**长度**和**方向**：

若：
$$
\vec{a} = (1, 1)
$$  
则：
$$
2\vec{a} = (2, 2), \quad -2\vec{a} = (-2, -2)
$$

---

#### 2.3.2 向量的内积（点积）

两个向量的乘积若结果为**标量**，称为内积（点积），记为：
$$
\vec{a} \cdot \vec{b}
$$

计算公式：
$$
\vec{a} \cdot \vec{b} = a_1b_1 + a_2b_2 + \dots + a_nb_n
$$

与夹角关系：
$$
\vec{a} \cdot \vec{b} = |\vec{a}| |\vec{b}| \cos\theta
$$

---
#### 2.3.3 向量的外积（叉积）

外积结果为**向量**，仅在三维空间中定义。  
记为：
$$
\vec{a} \times \vec{b}
$$

性质：
- 结果向量垂直于 $\vec{a}$ 和 $\vec{b}$ 所在平面。
- 其大小等于两向量所构成平行四边形的面积。

设：

$$
\vec{a} = (a_1, a_2, a_3), \ \vec{b} = (b_1, b_2, b_3)
$$

则：

$$
\vec{a} \times \vec{b} =
\text{det}\begin{bmatrix}
i & j & k \\
a_1 & a_2 & a_3 \\
b_1 & b_2 & b_3
\end{bmatrix}
\approx
|\, i\ j\ k;\ a_1\ a_2\ a_3;\ b_1\ b_2\ b_3\,|
$$


---

## 3. 矩阵（Matrix）

### 3.1 定义

矩阵是一个由数字组成的二维数组，用**大写字母**表示（如 A）。  
$m×n$ 矩阵有 m 行 n 列：

$$
A =
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{bmatrix}
$$

---

### 3.2 特殊矩阵类型

| 名称 | 特点 |
|------|------|
| 行矩阵 | 只有一行 |
| 列矩阵 | 只有一列 |
| 零矩阵 | 全为 0 |
| 方阵 | 行数 = 列数 |
| 单位矩阵 | 对角线为 1，其余为 0 |
| 同型矩阵 | 行列数相同的矩阵 |

---

### 3.3 矩阵的加减法

两个矩阵 **必须同型**。  
结果为对应位置元素相加减。

$$
(A+B)_{ij} = a_{ij} + b_{ij}
$$

---

### 3.4 矩阵的乘法

#### 3.4.1 数乘

矩阵所有元素乘以常数 $k$：

$$
kA =
\begin{bmatrix}
ka_{11} & ka_{12} \\
ka_{21} & ka_{22}
\end{bmatrix}
$$

---

#### 3.4.2 矩阵内积（乘法）

若矩阵 $A_{m×n}$ 与 $B_{n×p}$ 可乘，结果为 $C_{m×p}$：

$$
C = AB
$$  
其中：
$$
c_{ij} = \sum_{k=1}^{n} a_{ik}b_{kj}
$$

⚠️ 注意：矩阵乘法**不满足交换律**。

---

### 3.5 矩阵的转置

将矩阵的行列互换。  
若：
$$
A =
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
$$  
则：
$$
A^T =
\begin{bmatrix}
1 & 3 \\
2 & 4
\end{bmatrix}
$$

---

### 3.6 小结

- **标量**：单个数值  
- **向量**：有方向和大小  
- **矩阵**：二维数组  
- 矩阵可视作多个行或列向量的集合

---

## 4. 张量（Tensor）

张量是具有**任意维度**的数组：
- 0 维：标量  
- 1 维：向量  
- 2 维：矩阵  
- 高维：多维数组（如图像、视频等）

---

### 张量在人工智能中的应用

1. **数据表示**
   - 图像：三维张量（高度 × 宽度 × 通道）
   - 视频：四维张量（帧 × 高 × 宽 × 通道）
   - 音频：一维张量（时间序列）
   - 文本：二维张量（词嵌入矩阵）

2. **神经网络参数**
   - 权重和偏置以张量形式存储。

3. **运算与变换**
   - 矩阵乘法（全连接层）
   - 卷积（CNN）
   - 池化（降维）

4. **激活函数**
   - 对输入张量进行非线性变换。

5. **优化与反向传播**
   - 梯度也是张量形式。

6. **GPU 并行计算**
   - 张量运算可在 GPU 上高效执行。

---

### 💡 总结

张量是人工智能和深度学习的**核心数据结构**，所有的模型训练、权重更新、特征提取都基于张量计算。  
常用框架如 **TensorFlow、PyTorch、PaddlePaddle** 等，都围绕张量运算进行设计。

---
📄 **整理者**：*Hao nan Yu*   
🕒 更新时间：2025年10月
